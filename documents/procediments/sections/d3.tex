\begin{figure}[ht!]
\centering
\includegraphics[width=65mm]{data/car1.png}
\caption{Els sensors detecten la distància del cotxe a les parets del circuit.}
\label{sensors}
\end{figure}

La nostra segona demostració és radicalment diferent a la primera:
l'objectiu és programar un automòbil el qual, a partir de tres sensors frontals,
sigui capaç d'evitar xocar amb les parets de \emph{qualsevol circuit}.
És a dir; el seu entrenament ha de ser general i adaptable a qualsevol situació.

Per a solucionar un problema com aquest, no disposem d'un \emph{dataset},
com en el cas del reconeixement de caràcters: qui s'ha dedicat a crear
una base de dades que representa totes les decisions que hauria de prendre
el cotxe, en qualsevol dels casos en que es pot trobar? Evidentment, no és
una tàctica adequada.

Hem d'atacar el problema des d'un altre punt de vista: ara ja no es tracta
d'una situació en la qual l'entrenament de l'agent es pugui supervisar
(\emph{supervised learning}), ara ha d'ésser ell mateix qui explori l'entorn
en el qual es troba. Utilitzarem el segon mètode més popular d'aprenentatge
de màquina: l'aprenentatge sense supervisió (\emph{unsupervised learning}, o \emph{reinforcement learning}).
\\

\section{Reinforcement Learning}

\begin{figure}[ht!]
\centering
\includegraphics[width=52mm]{data/reinforce.png}
\caption{Diagrama que representa l'aprenentatge per reforç.}
\label{reinforcegraph}
\end{figure}

L'\emph{aprenentatge per reforç} es basa en la presa de decisions de l'agent en base
a una prèvia exploració de l'entorn, la qual ha sigut controlada a través d'un sistema
de recompenses \cite{reinflnbk}.

Ho podem il·lustrar de forma senzilla: un ratolí representa el nostre agent. Aquest divaga
de forma aleatòria a través d'un entorn desconegut. Quan cau en una trampa, se li dona una
recompensa negativa; quan es troba amb un tros de formatge, una de positiva. Poc a poc,
el ratolí aprendrà a evitar trampes i buscar el formatge.

Expressat amb termes més tècnics: el ratolí es troba en un estat \(s\) (que
emmagatzema la quantitat de peces de formatge que té, i si està viu o no), i
realitza una acció \(a\). Això el porta a trobar-se en un estat \(s'\).
Es fa llavors una comparació entre els dos estats, \(s\) i \(s'\), i se
li dona una recompensa \(r\) en base a uns criteris (per cada tros de formatge
que hagi guanyat, \(+5\); si segueix viu, \(+1\); si ha caigut en una trampa,
\(-100\)).

Aquesta és l'essència
de l'aprenentatge per reforç.

\section{Q-Learning}

\emph{Q-learning} és un algoritme d'aprenentatge per esforç. Expressat de forma
matemàtica:

\begin{equation} \label{eq:qlearning}
Q[s, a] = Q[s, a] + \alpha(r + \lambda V(s') - Q[s, a])
\end{equation}

Però que no espanti massa. En realitat, és extremadament fàcil d'implementar (en 
comparació a l'ús d'una \ac{ann}; no hem ni escrit les fórmules adients, degut a la seva
complexitat). Anem pas per pas, seguint el camí que hem fet per a arribar a 
un programa funcional.

Primer de tot, hem de mesurar l'estat en que es troba el nostre agent. Aquesta mesura ha de tenir les dades
que siguin necessàries per a corregir l'agent. Ens interessa, principalment, la distància
a les parets del circuit. Per a fer-ho efectivament, hem projectat tres \emph{làsers} o
sensors, que s'allarguen fins que el color sobre el qual es troben no és el color del circuit.
Un làser va en direcció igual a la del cotxe, i els altres dos es desvien 15 graus per a obtenir
una visió panoràmica de la posició del vehicle \ref{sensors}. Dibuixar les línies que
representen els sensors ha sigut tot un repte en quant a programació i matemàtiques.

Ara ja podem calcular la llargada dels làsers; és això tot el que necessitem per mesurar l'estat
en que es troba el vehicle i donar-li una recompensa. Però, l'algoritme \emph{Q-learning} té una limitació: la seva aplicació
comporta la creació d'un \emph{array} \footnote{També conegut com a \emph{vector}: llista de valors}
que tindrà una quantitat d'entrades igual a \emph{totes les possibles combinacions d'estat}. És a dir,
en el cas dels sensors del cotxe, que poden arribar a tenir un valor de 1,000 (píxels), l'array
tindrà \num{1000000000} entrades (1000 valors possibles per sensor, multiplicat per si mateix tres vegades, la quantitat
de sensors que tenim). No és viable; hem de \emph{discretitzar} una magnitud que, a efectes pràctics, és contínua.

Per tant, volem convertir el valor del sensor a una categoria del 0 al 9, on 0 significa ``molt a prop'', i 9 ``molt lluny'',
per a poder crear un array de dimensions viables (en aquest cas, tenim 10 estats per sensor, per tant, 1000 estats en total).
Com que ens interessa entrenar amb més precisió els casos en que els sensors tenen distàncies molt curtes (el cotxe està a 
prop de la paret), ens agradaria transformar de forma no simètrica els valors dels sensors, és a dir, que un valor real de
250px es transformi a 5, per exemple, i no a 2.5
Primer vam intentar realitzar aquest procés de discretització a partir d'una transformació logarítmica:
\begin{equation}
s_i(D) = \left\lfloor 9 \frac{ln(D)}{ln(1000)} \right\rfloor
\end{equation}
però els valors resultants estaven massa desproporcionats. Vam decidir fer-ho a la força bruta, establint condicions
simples. No és la solució més elegant, però no podem perdre temps amb qüestions trivials.

L'altra qüestió important és la \emph{taula de recompenses}, que defineix com serà recompensat l'agent. En base a què
hauríem d'establir els valors de la taula? L'impuls primitiu és ben simple: recompensa positiva si l'agent segueix viu,
i recompensa negativa si l'agent ha mort. Però això presenta un problema: encara que l'agent s'aproximi perillosament 
a una paret, la recompensa seguirà essent positiva. Així doncs, veiem que hem de basar la recompensa en la distància dels sensors:
com més curta sigui, recompensa més negativa tindrà. Tot i això, introduïm un altre problema: a mesura que l'agent s'aproximi a una
corba, la recompensa es començarà a reduir, i arribarà un moment que es veurà forçat a girar cua. Finalment, hem arribat a una conclusió
no massa intuïtiva: l'agent haurà de rebre recompenses positives (decreixents, però positives) encara que s'apropi a una paret, per a
afavorir l'exploració de l'entorn. Per tant, la \(r\) de l'equació \ref{eq:qlearning} queda definida
com a la ``suma de les recompenses que reben tots els sensors (3)''. Aquí la taula:

\begin{center} \label{tb:rewards}
    \begin{tabular}{| l | l |}
    \hline
    \(S_i\) & \(r\) \\ \hline
    0 & -10 \\ \hline
    1 & -5 \\ \hline
    2 & -1 \\ \hline
    3 & 0 \\ \hline
    4 & 1 \\ \hline
    5 & 2 \\ \hline
    6 & 3 \\ \hline
    7 & 4 \\ \hline
    8 & 5 \\ \hline
    9 & 6 \\ \hline
    \end{tabular}
\end{center}

Ja només queda establir els valors de les constants \(\alpha\) (proporció d'aprenentatge) i \(\lambda\) (factor de descompte), que equivaldran
a \(0.7\) i \(1\) respectivament.

El nostre agent realitza un \emph{tick} \footnote{Cicle} d'aprenentatge cada \(0.5\) segons, i triga al voltant de tres hores a ajustar els
valors de l'array Q per a funcionar amb desimboltura.

Com podríem millorar el funcionament d'aquesta demo? Principalment, de dos formes:

\begin{enumerate}
\item \emph{Selecció genètica}: fer funcionar a un set de \(n\) agents alhora, i guardar els arrays Q que millor hagin progressat durant
una sessió d'entrenament.
\item \emph{Accelerar el temps}: si ajustem l'escala temporal de forma completa (és a dir; si es mou 2 vegades més ràpid, el tick d'aprenentatge
ha d'ésser 2 vegades més ràpid) podrem accelerar el procés d'aprenentatge.
\end{enumerate}













